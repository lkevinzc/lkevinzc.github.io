---
layout: post
title: "坑记"
author: "Liu Zichen"
categories: journal
tags: [dl,cv,random,thoughts]
abstract: 前路漫漫，踩坑难免，于此以鉴。
---

### 深度学习
##### 细节
- 训练时常常会用Combined *Sigmoid* + *BCELoss*，因为在数值计算上更稳定。那么在推理时一定不要忘记加上Sigmoid layer以得到正确范围的预测。
- 不同的Dataloader实现对背景的表示不一样（e.g. Maskrcnn_benchmark里用0，Detectron里用num_class+1）。如果弄错会导致训练时挑出来的前景样例完全不对，进而很可能计算出来的loss变成nan的情况。
- 目标检测器做**推理**的时候，一定一定一定要在送进NMS之前把低可能性的预测过滤掉。一定。这是个很蠢但是折腾了很久的坑。
- 语义分割的类数一定是num_class+1，背景是算作一类一并加入CE计算的；目标检测往往是直接用num_class，因为分类时每一个类都是分别用BCE来预测，而回归时是只关心前景的。
