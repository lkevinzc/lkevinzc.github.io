---
layout: post
title: "坑记"
author: "Liu Zichen"
categories: journal
tags: [dl,cv,random,thoughts]
abstract: 前路漫漫，踩坑难免，于此以鉴。
---

---

### 深度学习
##### 细节
- 训练时常常会用Combined *Sigmoid* + *BCELoss*，因为在数值计算上更稳定。那么在推理时一定不要忘记加上Sigmoid layer以得到正确范围的预测。
- 不同的Dataloader实现对背景的表示不一样（e.g. Maskrcnn_benchmark里用0，Detectron里用num_class+1）。如果弄错会导致训练时挑出来的前景样例完全不对，进而很可能计算出来的loss变成nan的情况。
- 目标检测器做**推理**的时候，一定一定一定要在送进NMS之前把低可能性的预测过滤掉。一定。这是个很蠢但是折腾了很久的坑。
- 语义分割的类数一定是num_class+1，背景是算作一类一并加入CE计算的；目标检测往往是直接用num_class，因为分类时每一个类都是分别用BCE来预测，而回归时是只关心前景的。
- 不同的pretrained models (from MS, caffe, pytorch_model_zoo etc.) 做image normalization的方式可能是不一样的，要选择符合的方式更利于Transfer learning。
- 做evaluation时需特别注意，原图的大小和送进模型的大小一般都不一样，出了结果之后需要re-scale回原图（Box，Mask，等等都是）再做evaluation。
- 对于回归预测，initialization的参数尽可能小，不然容易导致loss NaN。比如fvcore.nn.weight_init.c2\_msra\_fill(conv)就不太合适；而应该用torch.nn.init.normal\_(conv.weight, std=0.01)。
- Selective Loss应该作用在倒数第二个stage，因为这个BUG浪费了两次实验，因此一定要细心谨慎，先想清楚再做。
- If use .sqrt() to describe loss, remember to use (+epsilon).sqrt() to avoid NaN.
- Remember to use torch.rand(size, device=others.device) **forever** instead of torch.rand(size).to(others.device); magnitudes of speed-up!
- PyTorch 的DDP目前（April， 2020）看样子做的不够好，两卡能跑的代码放到另一台机器四卡去跑总是报tcp通信的错，浪费了很多时间折腾，本以为是机器环境问题或者卡的架构问题，结果最后直接降回两卡又能跑了。反思一下，不要在一个问题上纠结太久去直接解决问题本身，视野放宽或许能更快找到替代方案。


### Misc.
- 写入lmdb数据时提前排好序非常重要！随机的index写入会导致整个程序非常非常慢（数据量大时）。